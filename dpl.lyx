#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 606
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ctex-report
\begin_preamble
% 如果没有这一句命令，XeTeX会出错，原因参见
% http://bbs.ctex.org/viewthread.php?tid=60547
\DeclareRobustCommand\nobreakspace{\leavevmode\nobreak\ }

\usepackage{color}
\usepackage{listings}
\lstset{ %
language=Python,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=4,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}
%\lstset{     
%basicstyle=\tt,        
%showstringspaces=false, 
%} 
%\begin{lstlisting}[language=C]       
%#include <stdio.h> {     
%//Example     
%printf("Hello World");     
%int i, sum = 0;     
%for (i = 0; i < 10; i++){
%        sum += i;     
%}     
%printf("%d", sum);     
%return 0; } 
%\end{lstlisting}
\usepackage{hyperref}

\setmainfont{MI~Lan~Pro~VF~Light}
\setCJKmainfont{MI~Lan~Pro~VF~Light}[BoldFont={MI~Lan~Pro~VF~Medium}, ItalicFont={MI~Lan~Pro~VF~Thin}]
\usepackage[UTF8,nofonts]{ctexcap}
\end_preamble
\options UTF8
\use_default_options true
\maintain_unincluded_children no
\language chinese-simplified
\language_package none
\inputencoding utf8-plain
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format pdf4
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\end_header

\begin_body

\begin_layout Title
深度学习简单教程
\end_layout

\begin_layout Author
秦以南
\end_layout

\begin_layout Abstract
这个教程是由我在学习Deep Learning的过程中写成的，现在分享给大家，希望大家可以更好的理解它~
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
机器学习和深度学习综述
\end_layout

\begin_layout Section
人工智能、机器学习、深度学习的关系
\end_layout

\begin_layout Standard
近些年人工智能、机器学习和深度学习的概念十分火热，但很多从业者却很难说清它们之间的关系，外行人更是雾里看花。在研究深度学习之前，先从三个概念的正本清源开始。概括
来说，人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的，三者的关系如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示，即：人工智能 > 机器学习 > 深度学习。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/1.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
人工智能、机器学习和深度学习三者关系示意
\begin_inset CommandInset label
LatexCommand label
name "fig:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
人工智能（ArtificialIntelligence，AI）是最宽泛的概念，是研发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。
由于这个定义只阐述了目标，而没有限定方法，因此实现人工智能存在的诸多方法和分支，导致其变成一个“大杂烩”式的学科。机器学习（MachineLearning，ML
）是当前比较有效的一种实现人工智能的方式。深度学习（DeepLearning，DL）是机器学习算法中最热门的一个分支，近些年取得了显著的进展，并替代了大多数传统
机器学习算法。
\end_layout

\begin_layout Section
机器学习
\end_layout

\begin_layout Standard
区别于人工智能，机器学习、尤其是监督学习则有更加明确的指代。机器学习是专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，
使之不断改善自身的性能。这句话有点“云山雾罩”的感觉，让人不知所云，下面我们从机器学习的实现和方法论两个维度进行剖析，帮助读者更加清晰地认识机器学习的来龙去脉。
\end_layout

\begin_layout Subsection
机器学习的实现
\end_layout

\begin_layout Standard
机器学习的实现可以分成两步：训练和预测，类似于归纳和演绎：
\end_layout

\begin_layout Itemize
归纳： 从具体案例中抽象一般规律，机器学习中的“训练”亦是如此。从一定数量的样本（已知模型输入
\begin_inset Formula $X$
\end_inset

和模型输出
\begin_inset Formula $Y$
\end_inset

）中，学习输出
\begin_inset Formula $\bar{{Y}}$
\end_inset

与输入
\begin_inset Formula $\bar{{X}}$
\end_inset

的关系（可以想象成是某种表达式）。
\end_layout

\begin_layout Itemize
演绎： 从一般规律推导出具体案例的结果，机器学习中的“预测”亦是如此。基于训练得到的
\begin_inset Formula $Y$
\end_inset

与
\begin_inset Formula $X$
\end_inset

之间的关系，如出现新的输入
\begin_inset Formula $X$
\end_inset

，计算出输出
\begin_inset Formula $Y$
\end_inset

。通常情况下，如果通过模型计算的输出和真实场景的输出一致，则说明模型是有效的。
\end_layout

\begin_layout Subsection
机器学习的方法论
\end_layout

\begin_layout Standard
机器学习的方法论和人类科研的过程有着异曲同工之妙，下面以“机器从牛顿第二定律实验中学习知识”为例，帮助读者更加深入理解机器学习（监督学习）的方法论本质，即在“机
器思考”的过程中确定模型的三个关键要素：假设、评价、优化。
\end_layout

\begin_layout Subsubsection
案例：机器从牛顿第二定律实验中学习知识
\end_layout

\begin_layout Paragraph
牛顿第二定律
\end_layout

\begin_layout Standard
牛顿第二定律是艾萨克·牛顿在1687年于《自然哲学的数学原理》一书中提出的，其常见表述：物体加速度的大小跟作用力成正比，跟物体的质量成反比，与物体质量的倒数成正
比。牛顿第二运动定律和第一、第三定律共同组成了牛顿运动定律，阐述了经典力学中基本的运动规律。
\end_layout

\begin_layout Standard
在中学课本中，牛顿第二定律有两种实验设计方法：倾斜滑动法和水平拉线法，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.2"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/2.jpg
	lyxscale 90
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
牛顿第二定律实验设计方法
\begin_inset CommandInset label
LatexCommand label
name "fig:1.2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
通过多次实验数据，可以统计出如表
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:1.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示的不同作用力下的木块加速度。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float table
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
项目
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
作用力
\begin_inset Formula $X$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
加速度
\begin_inset Formula $Y$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
第1次
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
第2次
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\cdots$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
第n次
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
实验获取的大量数据样本和观测结果
\begin_inset CommandInset label
LatexCommand label
name "tab:1.1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
观察实验数据不难猜测，物体的加速度
\begin_inset Formula $a$
\end_inset

和作用力
\begin_inset Formula $F$
\end_inset

之间的关系应该是线性关系。因此我们提出假设 
\begin_inset Formula $a=w\cdot F$
\end_inset

，其中，
\begin_inset Formula $a$
\end_inset

代表加速度，
\begin_inset Formula $F$
\end_inset

代表作用力，
\begin_inset Formula $w$
\end_inset

是待确定的参数。
\end_layout

\begin_layout Standard
通过大量实验数据的训练，确定参数
\begin_inset Formula $w$
\end_inset

是物体质量的倒数
\begin_inset Formula $(1/m)$
\end_inset

，即得到完整的模型公式
\begin_inset Formula $a=F\cdot(1/m)$
\end_inset

。当已知作用到某个物体的力时，基于模型可以快速预测物体的加速度。例如：燃料对火箭的推力
\begin_inset Formula $F=10$
\end_inset

，火箭的质量
\begin_inset Formula $m=2$
\end_inset

，可快速得出火箭的加速度
\begin_inset Formula $a=5$
\end_inset

。
\end_layout

\begin_layout Subsubsection
如何确定模型参数？
\end_layout

\begin_layout Standard
这个有趣的案例演示了机器学习的基本过程，但其中有一个关键点的实现尚不清晰，即：
\series bold
如何确定模型参数
\series default

\begin_inset Formula $（w=1/m）$
\end_inset

？
\end_layout

\begin_layout Standard
确定参数的过程与科学家提出假说的方式类似，合理的假说可以最大化的解释所有的已知观测数据。如果未来观测到不符合理论假说的新数据，科学家会尝试提出新的假说。如：天文
史上，使用大圆和小圆组合的方式计算天体运行，在中世纪是可以拟合观测数据的。但随着欧洲工业革命的推动，天文观测设备逐渐强大，已有的理论已经无法解释越来越多的观测数
据，这促进了使用椭圆计算天体运行的理论假说出现。因此，
\series bold
模型有效的基本条件是能够拟合已知的样本
\series default
，这给我们提供了学习有效模型的实现方案。
\end_layout

\begin_layout Standard
图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.3"
plural "false"
caps "false"
noprefix "false"

\end_inset

是以
\begin_inset Formula $H$
\end_inset

为模型的假设，它是一个关于参数
\begin_inset Formula $w$
\end_inset

和输入
\begin_inset Formula $x$
\end_inset

的函数，用
\begin_inset Formula $H(w,x)$
\end_inset

表示。模型的优化目标是
\begin_inset Formula $H(w,x)$
\end_inset

的输出与真实输出
\begin_inset Formula $Y$
\end_inset

尽量一致，两者的相差程度即是模型效果的评价函数（相差越小越好）。那么，确定参数的过程就是在已知的样本上，不断减小该评价函数（
\begin_inset Formula $H$
\end_inset

和
\begin_inset Formula $Y$
\end_inset

的差距）的过程。直到模型学习到一个参数
\begin_inset Formula $w$
\end_inset

，使得评价函数的值最小，
\series bold
衡量模型预测值和真实值差距的评价函数也被称为损失函数（损失Loss）
\series default
。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/3.jpg
	lyxscale 60
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
确定模型参数示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:1.3"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
假设机器通过尝试答对（最小化损失）大量的习题（已知样本）来学习知识（模型参数
\begin_inset Formula $w$
\end_inset

），并期望用学习到的知识所代表的模型
\begin_inset Formula $H(w,x)$
\end_inset

，回答不知道答案的考试题（未知样本）。最小化损失是模型的优化目标，实现损失最小化的方法称为优化算法，也称为寻解算法（找到使得损失函数最小的参数解）。参数
\begin_inset Formula $w$
\end_inset

和输入
\begin_inset Formula $x$
\end_inset

组成公式的基本结构称为假设。在牛顿第二定律的案例中，基于对数据的观测，我们提出了线性假设，即作用力和加速度是线性关系，用线性方程表示。由此可见，
\series bold
模型假设、评价函数（损失/优化目标）和优化算法是构成模型的三个关键要素
\series default
。
\end_layout

\begin_layout Subsubsection
模型结构
\end_layout

\begin_layout Standard
模型假设、评价函数和优化算法是如何支撑机器学习流程的呢？如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.4"
plural "false"
caps "false"
noprefix "false"

\end_inset

 所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/4.png
	lyxscale 25
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
机器学习流程
\begin_inset CommandInset label
LatexCommand label
name "fig:1.4"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
模型假设
\series default
：世界上的可能关系千千万，漫无目标的试探
\begin_inset Formula $Y\sim X$
\end_inset

之间的关系显然是十分低效的。因此假设空间先圈定了一个模型能够表达的关系可能，如蓝色圆圈所示。机器还会进一步在假设圈定的圆圈内寻找最优的
\begin_inset Formula $Y\sim X$
\end_inset

关系，即确定参数
\begin_inset Formula $w$
\end_inset

。
\end_layout

\begin_layout Itemize

\series bold
评价函数
\series default
：寻找最优之前，我们需要先定义什么是最优，即评价一个
\begin_inset Formula $Y\sim X$
\end_inset

关系的好坏的指标。通常衡量该关系是否能很好的拟合现有观测样本，将拟合的误差最小作为优化目标。
\end_layout

\begin_layout Itemize

\series bold
优化算法
\series default
：设置了评价指标后，就可以在假设圈定的范围内，将使得评价指标最优（损失函数最小/最拟合已有观测样本）的
\begin_inset Formula $Y\sim X$
\end_inset

关系找出来，这个寻找最优解的方法即为优化算法。最笨的优化算法即按照参数的可能，穷举每一个可能取值来计算损失函数，保留使得损失函数最小的参数作为最终结果。
\end_layout

\begin_layout Standard
从上述过程可以得出，机器学习的过程与牛顿第二定律的学习过程基本一致，都分为假设、评价和优化三个阶段：
\end_layout

\begin_layout Enumerate

\series bold
假设
\series default
：通过观察加速度
\begin_inset Formula $a$
\end_inset

和作用力
\begin_inset Formula $F$
\end_inset

的观测数据，假设
\begin_inset Formula $a$
\end_inset

和
\begin_inset Formula $F$
\end_inset

是线性关系，即
\begin_inset Formula $a=w\cdot F$
\end_inset

。
\end_layout

\begin_layout Enumerate

\series bold
评价
\series default
：对已知观测数据上的拟合效果好，即
\begin_inset Formula $w\cdot F$
\end_inset

计算的结果要和观测的
\begin_inset Formula $a$
\end_inset

尽量接近。
\end_layout

\begin_layout Enumerate

\series bold
优化
\series default
：在参数
\begin_inset Formula $w$
\end_inset

的所有可能取值中，发现
\begin_inset Formula $w=1/m$
\end_inset

可使得评价最好（最拟合观测样本）。
\end_layout

\begin_layout Standard
机器执行学习任务的框架体现了其
\series bold
学习的本质是“参数估计”
\series default
（Learning is parameter estimation）。
\end_layout

\begin_layout Standard
上述方法论使用更规范化的表示如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.5"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示，未知目标函数
\begin_inset Formula $f$
\end_inset

，以训练样本
\begin_inset Formula ${D}=（{x_{1}}，{y_{1}}），…，（{x_{n}}，{y_{n}}$
\end_inset

）为依据。从假设集合
\begin_inset Formula $H$
\end_inset

中，通过学习算法
\begin_inset Formula $A$
\end_inset

找到一个函数
\begin_inset Formula $g$
\end_inset

。如果
\begin_inset Formula $g$
\end_inset

能够最大程度的拟合训练样本
\begin_inset Formula $D$
\end_inset

，那么可以认为函数
\begin_inset Formula $g$
\end_inset

就接近于目标函数
\begin_inset Formula $f$
\end_inset

。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/5.png
	lyxscale 65
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
规范化表示
\begin_inset CommandInset label
LatexCommand label
name "fig:1.5"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
在此基础上，许多看起来完全不一样的问题都可以使用同样的框架进行学习，如科学定律、图像识别、机器翻译和自动问答等，它们的学习目标都是拟合一个“大公式f”，如
 图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.6"
plural "false"
caps "false"
noprefix "false"

\end_inset

 所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/6.png
	lyxscale 60
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
机器学习就是拟合一个“大公式” 
\begin_inset CommandInset label
LatexCommand label
name "fig:1.6"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
深度学习
\end_layout

\begin_layout Standard
机器学习算法理论在上个世纪90年代发展成熟，在许多领域都取得了成功，但平静的日子只延续到2010年左右。随着大数据的涌现和计算机算力提升，深度学习模型异军突起，
极大改变了机器学习的应用格局。今天，多数机器学习任务都可以使用深度学习模型解决，尤其在语音、计算机视觉和自然语言处理等领域，深度学习模型的效果比传统机器学习算法
有显著提升。
\end_layout

\begin_layout Standard
相比传统的机器学习算法，深度学习做出了哪些改进呢？其实
\series bold
两者在理论结构上是一致的，即：模型假设、评价函数和优化算法，其根本差别在于假设的复杂度
\series default
。如 图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.6"
plural "false"
caps "false"
noprefix "false"

\end_inset

 第二个示例（图像识别）所示，对于美女照片，人脑可以接收到五颜六色的光学信号，能快速反应出这张图片是一位美女，而且是程序员喜欢的类型。但对计算机而言，只能接收到
一个数字矩阵，对于美女这种高级的语义概念，从像素到高级语义概念中间要经历的信息变换的复杂性是难以想象的，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.7"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/7.png
	lyxscale 80
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
深度学习的模型复杂度难以想象
\begin_inset CommandInset label
LatexCommand label
name "fig:1.7"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
这种变换已经无法用数学公式表达，因此研究者们借鉴了人脑神经元的结构，设计出神经网络的模型，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.8"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示。图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.8"
plural "false"
caps "false"
noprefix "false"

\end_inset

（a）展示了神经网络基本单元-感知机的设计方案，其处理信息的方式与人脑中的单一神经元有很强的相似性；图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.8"
plural "false"
caps "false"
noprefix "false"

\end_inset

（b）展示了几种经典的神经网络结构（后续的章节中会详细阐述），类似于人脑中多种基于大量神经元连接而形成的不同职能的器官。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/8.png
	lyxscale 60
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
模拟人脑结构，针对各种任务设计不同的深度学习模型
\begin_inset CommandInset label
LatexCommand label
name "fig:1.8"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
神经网络的基本概念
\end_layout

\begin_layout Standard
人工神经网络包括多个神经网络层，如：卷积层、全连接层、LSTM等，每一层又包括很多神经元，超过三层的非线性神经网络都可以被称为深度神经网络。通俗的讲，深度学习的
模型可以视为是输入到输出的映射函数，如图像到高级语义（美女）的映射，足够深的神经网络理论上可以拟合任何复杂的函数。因此神经网络非常适合学习样本数据的内在规律和表
示层次，对文字、图像和语音任务有很好的适用性。这几个领域的任务是人工智能的基础模块，因此深度学习被称为实现人工智能的基础也就不足为奇了。
\end_layout

\begin_layout Standard
神经网络基本结构如 图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.9"
plural "false"
caps "false"
noprefix "false"

\end_inset

 所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/9.png
	lyxscale 60
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
神经网络基本结构示意图
\begin_inset CommandInset label
LatexCommand label
name "fig:1.9"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
神经元： 神经网络中每个节点称为神经元，由两部分组成：
\end_layout

\begin_deeper
\begin_layout Itemize
加权和：将所有输入加权求和。
\end_layout

\begin_layout Itemize
非线性变换（激活函数）：加权和的结果经过一个非线性函数变换，让神经元计算具备非线性的能力。
\end_layout

\end_deeper
\begin_layout Itemize
多层连接： 大量这样的节点按照不同的层次排布，形成多层的结构连接起来，即称为神经网络。
\end_layout

\begin_layout Itemize
前向计算： 从输入计算输出的过程，顺序从网络前至后。
\end_layout

\begin_layout Itemize
计算图： 以图形化的方式展现神经网络的计算逻辑又称为计算图，也可以将神经网络的计算图以公式的方式表达：
\end_layout

\begin_layout Standard
\align center
\begin_inset Formula $Y=f_{3}(f_{2}(f_{1}(w_{1}\cdot x_{1}+w_{2}\cdot x_{2}+w_{3}\cdot x_{3}+b)+…)…)…)$
\end_inset


\end_layout

\begin_layout Standard
由此可见，神经网络并没有那么神秘，它的本质是一个含有很多参数的“大公式”。
\end_layout

\begin_layout Subsection
深度学习的发展历程
\end_layout

\begin_layout Standard
神经网络思想的提出已经是70多年前的事情了，现今的神经网络和深度学习的设计理论是一步步趋于完善的。在这漫长的发展岁月中，一些取得关键突破的闪光时刻，值得深度学习
爱好者们铭记，如 图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:1.10"
plural "false"
caps "false"
noprefix "false"

\end_inset

 所示。
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter1/10.png
	lyxscale 60
	scale 28

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
深度学习发展历程
\begin_inset CommandInset label
LatexCommand label
name "fig:1.10"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
1940年代
\series default
：首次提出神经元的结构，但权重是不可学的。
\end_layout

\begin_layout Itemize

\series bold
50-60年代
\series default
：提出权重学习理论，神经元结构趋于完善，开启了神经网络的第一个黄金时代。
\end_layout

\begin_layout Itemize

\series bold
1969年
\series default
：提出异或问题（人们惊讶的发现神经网络模型连简单的异或问题也无法解决，对其的期望从云端跌落到谷底），神经网络模型进入了被束之高阁的黑暗时代。
\end_layout

\begin_layout Itemize

\series bold
1986年
\series default
：新提出的多层神经网络解决了异或问题，但随着90年代后理论更完备并且实践效果更好的SVM等机器学习模型的兴起，神经网络并未得到重视。
\end_layout

\begin_layout Itemize

\series bold
2010年左右
\series default
：深度学习进入真正兴起时期。随着神经网络模型改进的技术在语音和计算机视觉任务上大放异彩，也逐渐被证明在更多的任务，如自然语言处理以及海量数据的任务上更加有效。至
此，神经网络模型重新焕发生机，并有了一个更加响亮的名字：深度学习。
\end_layout

\begin_layout Standard
为何神经网络到2010年后才焕发生机呢？这与深度学习成功所依赖的先决条件：大数据涌现、硬件发展和算法优化有关。
\end_layout

\begin_layout Itemize

\series bold
大数据是神经网络发展的有效前提
\series default
。神经网络和深度学习是非常强大的模型，需要足够量级的训练数据。时至今日，之所以很多传统机器学习算法和人工特征依然是足够有效的方案，原因在于很多场景下没有足够的标
记数据来支撑深度学习。深度学习的能力特别像科学家阿基米德的豪言壮语：“给我一根足够长的杠杆，我能撬动地球！”。深度学习也可以发出类似的豪言：“给我足够多的数据，
我能够学习任何复杂的关系”。但在现实中，足够长的杠杆与足够多的数据一样，往往只能是一种美好的愿景。直到近些年，各行业IT化程度提高，累积的数据量爆发式地增长，才
使得应用深度学习模型成为可能。
\end_layout

\begin_layout Itemize

\series bold
依靠硬件的发展和算法的优化
\series default
。现阶段，依靠更强大的计算机、GPU、autoencoder预训练和并行计算等技术，深度学习在模型训练上的困难已经被逐渐克服。其中，数据量和硬件是更主要的原因。
没有前两者，科学家们想优化算法都无从进行。
\end_layout

\begin_layout Section
小结
\end_layout

\begin_layout Standard
虽然历史悠久，但深度学习在今天依然在蓬勃发展，一方面基础研究快速发展，另一方面工业实践层出不穷。基于深度学习的顶级会议ICLR(International
 Conference on Learning Representations)统计，深度学习相关的论文数量呈逐年递增的状态。同时，不仅仅是深度学习会议，与数据
和模型技术相关的会议ICML和KDD，专注视觉的CVPR和专注自然语言处理的EMNLP等国际会议的大量论文均涉及着深度学习技术。该领域和相关领域的研究方兴未艾，
技术仍在不断创新突破中。
\end_layout

\begin_layout Standard
另一方面，以深度学习为基础的人工智能技术，在升级改造众多的传统行业领域，存在极其广阔的应用场景。人工智能技术不仅可在众多行业中落地应用（广度），同时，在部分行业
（如安防、遥感、互联网、金融、工业等）已经实现了市场化变现和高速增长（深度），为社会贡献了巨大的经济价值。
\end_layout

\begin_layout Chapter
深度学习概念简介
\end_layout

\begin_layout Section
机器学习基本概念
\end_layout

\begin_layout Standard
由于深度学习是一种特殊的机器学习方法，一般的机器学习概念对于深度学习也适用。为了能够更好地对深度学习的一些概念做阐述，这里把机器学习的概念作 为独立的一节，希望
能够帮助读者更好地理解深度学习的基本概念。
\end_layout

\begin_layout Subsection
机器学习的主要任务
\end_layout

\begin_layout Standard
首先让我们了解一下机器学习的主要任务。一般来说，我们可以把这个任务描述为：给定在现实中获取的一系列
\series bold
数据
\series default
来构建一个
\series bold
模型
\series default
，用于描述数据的具体
\series bold
分布
\series default
（这个分布指的是概率论意义上的分布。也就是说，某些数据可能出现得比较频繁，另外一些数据可能出现得比较稀少，我们需要描述什么数据比较可能出现，并且用概率来描述数据
出现的可能性）。在这里用粗体标出了三个重要的词，下面详细介绍一下这些词的具体意义。
\end_layout

\begin_layout Standard

\series bold
数据
\series default
是人们对周围发生的一些现象的数字化描述。我们在现实中获取的数据是各种各样的，比如图片（计算机使用像素的排列来表示图片，其中每个像素可表示为三个数值（即红绿蓝（R
GB））或四个数值和透明度（RGBA））、文本（计算机使用字符串来表示文本）、音频（计算机中一个比较通用的格式是使用16bit（即65536个值）来代表声音的振
幅，按照一定采样频率如44.1kHz来组成一段音频。）等。
\end_layout

\begin_layout Standard
这些数据可以分成有标签的（比如电影评论的文本和文本对应的评价）和没有标签的（比如只是一段文本）。根据能够处理的数据类型，机器学习的任务大致可以分为两类：监督学习
（Supervised Learning）和无监督学习（Unsupervised Learning）。前者指的是，我们拥有具体的数据和预测目标（标签），需要从数
据出发构造具体的模型预测这些目标，当然模型预测越准确越好，其代表的主要任务包括回归（预测连续的值，如根据历史的气温预测未来的气温）和分类（预测离散的值，如根据图
片预测图片描述的具体物体的类型）。后者指的是，我们只有数据，没有预测目标，需要构造具体的模型来找出数据的具体规律，其代表的主要任务包括聚类（找出相似的多组数据并
把它们归类）。当然，实际上还有介于监督学习和无监督学习方法之间的 半监督学习（Semi-supervised Learning），这里限于篇幅，就不具体展开介绍
。
\end_layout

\begin_layout Standard

\series bold
模型
\series default
是另一个在机器学习中经常碰到的概念。大多数机器学习任务的目的可以归纳为得到一个模型，用这个模型来描述给定的数据。得到这个模型的过程称之为机器学习模型的训练过程。
从概率论的角度说，我们可以认为机器学习模型是一个
\series bold
概率分布 
\begin_inset Formula $P_{\theta}(X)$
\end_inset


\series default
（这里以概率密度函数来代表概率分布），其中 
\begin_inset Formula $X$
\end_inset

 是训练数据，这个概率分布拥有参数
\begin_inset Formula $\boldsymbol{\theta}=\left(\theta_{1},\theta_{2},\cdots,\theta_{n}\right)$
\end_inset

（这个参数可以是一个或多个，我们用粗体字母来代表所有的参数 ，
\begin_inset Formula $n$
\end_inset

代表参数的个数）。
\end_layout

\begin_layout Standard
机器学习的任务就是求最优参数
\begin_inset Formula $\boldsymbol{\theta_{t}}$
\end_inset

使得概率分布
\begin_inset Formula $\boldsymbol{P_{\theta}(X)}$
\end_inset

最大。我们可以用式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

来代表这个过程，其中
\begin_inset Formula $argmax$
\end_inset

函数代表的是取参数使得数据的概率密度最大。求解最优参数
\begin_inset Formula $\boldsymbol{\theta_{t}}$
\end_inset

的过程，我们称之为模型的训练过程（Training），用来训练这个模型的数据集称之为
\series bold
训练集
\series default
（Training Set），由此得到的模型就可以用来做相应的预测。
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta_{t}=}\underset{\theta}{argmax}\,\boldsymbol{P_{\theta}(X)}\label{eq:2.1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
由于用于机器学习的训练集通常有很多条具体数据，我们可以把训练数据 
\begin_inset Formula $\boldsymbol{X}$
\end_inset

拆成单条数据的集合，即
\begin_inset Formula $\boldsymbol{X=\boldsymbol{(X_{1,}X_{2,}\cdots,X_{n})}}$
\end_inset

，其中
\begin_inset Formula $n$
\end_inset

为数据的总数目，
\begin_inset Formula $X$
\end_inset

为数据的
\series bold
批次
\series default
（batch），
\begin_inset Formula $X$
\end_inset

的子集称之为数据的
\series bold
迷你批次
\series default
（mini-batch）。假设训练数据的单条数据之间相互独立，我们可以把式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

进一步展开成式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.2"
plural "false"
caps "false"
noprefix "false"

\end_inset

。需要注意的是，这里把对于独立单条数据的乘积取对数变成了求概率的对数的和，该方法是机器学习推导公式时经常使用的一个方法，我们把整体的求解最优参数
\begin_inset Formula $\boldsymbol{\theta_{t}}$
\end_inset

的过程称之为
\series bold
极大似然估计
\series default
（Maximum LikeIihood Estimation，MLE）。
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta_{t}=}\underset{\theta}{argmax}\,\boldsymbol{\underset{i}{\prod}P_{\theta}(X_{i})=\underset{\theta}{argmax}\,\boldsymbol{\underset{i}{\sum}log{P_{\theta}(X_{i})}}}\label{eq:2.2}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
机器模型的类型
\end_layout

\begin_layout Standard
以上的机器学习理论适用于所有的机器学习，包括监督学习和无监督学习。对于监督学习来说，我们面对的训练集包含输入的数据
\begin_inset Formula $X$
\end_inset

及其对应的标签
\begin_inset Formula $Y$
\end_inset

。所以我们应该求的是包含了
\begin_inset Formula $X$
\end_inset

和
\begin_inset Formula $Y$
\end_inset

的一个概率分布。根据概率论的知识可以知道，对应的概率分布（这里均以对应的概率密度函数来指代分布）有两种，即联合概率分布（Joint Distribution）
\begin_inset Formula $P_{\theta}(X,Y)$
\end_inset

和条件概率分布（Conditional Distribution）
\begin_inset Formula $P_{\theta}(Y|X)$
\end_inset

。即，前者表示的是数据和标签共同出现的概率，后者表示的是在给定数据的条件下，对应标签的概率。
\end_layout

\begin_layout Standard
根据描述的概率分布，我们也可以把监督学习的机器学习模型分成两类：生成式模型（Generative Model）和判别式模型（Discriminative
 Model），前者基于的是联合概率分布，后者基于的是条件概率分布。生成式模型代表性的有朴素贝叶斯（Naive Bayes，NB）和隐马尔可夫模型（Hidden
 Markov Model，HMM）等，判别式模型中具有代表性的有逻辑斯蒂回归（Logistic Regression，LR）和条件随机场（Conditiona
l Random Fields，CRF）等。
\end_layout

\begin_layout Standard
除了概率分布形式上的区别，这两类模型在许多方面都有区别。生成式模型除了能够根据输入数据
\begin_inset Formula $X$
\end_inset

来预测对应的标签
\begin_inset Formula $Y$
\end_inset

，还能根据训练得到的模型产生服从训练数据集分布的数据
\begin_inset Formula $(X,Y)$
\end_inset

，相当于生成一组新的数据，这个也就是名称中的生成式模型的来源。相对而言，判别式模型就仅能根据具体的数据
\begin_inset Formula $X$
\end_inset

来预测对应的标签
\begin_inset Formula $Y$
\end_inset

。一般来说，牺牲了生成数据的能力，判别式模型获取的是比生成式模型高的预测准确率。至于为什么判别式模型的预测准确率高，我们可以通过全概率公式和信息熵公式来说明这个
问题。首先看一下全概率公式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.3"
plural "false"
caps "false"
noprefix "false"

\end_inset

。我们可以看到，相对于条件概率，在计算联合概率的时候引入了输入数据的概率分布
\begin_inset Formula $P(X)$
\end_inset

，而这并不是我们关心的（我们只关心给定
\begin_inset Formula $X$
\end_inset

的情况下
\begin_inset Formula $Y$
\end_inset

的分布），于是这就相对削弱了模型的预测能力。
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(X,Y)=\int P(Y|X)P(X)dX\label{eq:2.3}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
损失函数以及模型的拟合
\end_layout

\begin_layout Standard
有了关于极大似然估计和监督学习的一些知识后，我们可以进一步考虑一个问题：
\series bold
如何优化机器学习模型
\series default
？
\end_layout

\begin_layout Standard
让我们首先从回归模型（Regression）出发。所谓回归模型，可以简单地认为拟合一个函数
\begin_inset Formula $f_{\theta}(x)$
\end_inset

.
 给定输入的值
\begin_inset Formula $\boldsymbol{X_{i}}$
\end_inset

，给出对应的目标值
\begin_inset Formula $\boldsymbol{Y_{i}}$
\end_inset

， 其中 
\begin_inset Formula $θ$
\end_inset

是需要训练的参数。假设我们的噪声，也就是模型的预测值和对应目标值差值 
\begin_inset Formula $\boldsymbol{f_{\theta}(X_{i})-Y_{i}}$
\end_inset

服从正态分布 
\begin_inset Formula $N(0,\sigma^{2})$
\end_inset

（P.S.
 关于正态分布的概念及性质请参考《概率论与数理统计》一书，如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2.1"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示）。这样就可以通过
\series bold
极大似然估计
\series default
的方法来计算最优的参数
\begin_inset Formula $\boldsymbol{\theta_{t}}$
\end_inset

。整个过程的公式如式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.4"
plural "false"
caps "false"
noprefix "false"

\end_inset

 ~式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.6"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示，其中，式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.4"
plural "false"
caps "false"
noprefix "false"

\end_inset

是根据极大似然估计，代入正态分布的概率密度函数，可以最后化简得到式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.5"
plural "false"
caps "false"
noprefix "false"

\end_inset

 。
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\boldsymbol{\theta_{t}}=\underset{\theta}{argmax}\underset{i}{\sum}log\boldsymbol{P(X_{i})}=-\underset{\theta}{argmax}\underset{i}{\sum}log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\boldsymbol{f_{\theta}(X_{i})-Y_{i}})^{2}}{2\sigma^{2}}}\label{eq:2.4}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\theta_{t}=\underset{\theta}{argmin}\sum_{i}\left(f_{\theta}\left(X_{i}\right)-Y_{i}\right)^{2}\label{eq:2.5}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P_{\theta}\left(X_{i}\right)=\mathrm{e}^{-\alpha\left|f_{\theta}\left(X_{i}\right)-Y_{i}\right|};\theta_{t}=\underset{\theta}{argmin}\sum_{i}\left|f_{\theta}\left(X_{i}\right)-Y_{i}\right|\label{eq:2.6}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
这个公式意味着，在上述的假设条件下，回归模型的极大估计可以归结为求出最优的参数
\begin_inset Formula $θ_{t}$
\end_inset

使得模型的预测值和目标值差值对于所有训练数据的平 方和最小。我们把argmin函数中需要优化的函数称之为
\series bold
损失函数
\series default
，这个损失函数被称为L2模损失函数（L2-norm Loss Function）或者方均误差函数（Mean Square Error, MSE）。我们可以看到这
个损失函数和正态分布密切相关。当然，如果误差服从其他分布，如拉普拉斯分布（Laplacian Distribution）（如图
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2.2"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示），最后得到的损失函数称为L1模损失函数（L1-norm Loss Function），如式
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2.6"
plural "false"
caps "false"
noprefix "false"

\end_inset

所示。
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter2/1.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
正态分布
\begin_inset CommandInset label
LatexCommand label
name "fig:2.1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement document
alignment document
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename imgs/chapter2/2.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
拉普拉斯分布
\begin_inset CommandInset label
LatexCommand label
name "fig:2.2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
